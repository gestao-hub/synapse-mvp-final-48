import { useEffect, useRef, useState } from "react";
import { supabase } from "@/integrations/supabase/client";

type StartOpts = { 
  track: "rh" | "comercial" | "educacional" | "gestao"; 
  scenario?: string; 
  systemPrompt?: string; 
  voiceId?: string; 
};

export function useRealtimeCall() {
  const pcRef = useRef<RTCPeerConnection | null>(null);
  const localStreamRef = useRef<MediaStream | null>(null);
  const remoteAudioRef = useRef<HTMLAudioElement | null>(null);
  const dataChannelRef = useRef<RTCDataChannel | null>(null);
  const [status, setStatus] = useState<"idle"|"connecting"|"connected"|"ended">("idle");
  const [muted, setMuted] = useState(false);
  const [currentSessionId, setCurrentSessionId] = useState<string | null>(null);
  const [userTranscript, setUserTranscript] = useState("");
  const [aiTranscript, setAiTranscript] = useState("");

  useEffect(() => {
    // cria <audio> invis√≠vel para tocar o lado remoto
    const el = document.createElement("audio");
    el.autoplay = true;
    el.style.display = "none";
    document.body.appendChild(el);
    remoteAudioRef.current = el;
    return () => { el.remove(); };
  }, []);

  // Fun√ß√£o para salvar transcri√ß√µes em tempo real
  const saveTranscript = async (text: string, speaker: 'user' | 'ai') => {
    if (!currentSessionId || !text?.trim()) return;
    
    try {
      console.log(`üìù Salvando transcript ${speaker}:`, text.substring(0, 50) + '...');
      
      const response = await supabase.functions.invoke('save-live-transcript', {
        body: {
          sessionId: currentSessionId,
          [speaker === 'user' ? 'userTranscript' : 'aiTranscript']: text,
          turnIndex: Date.now(),
          speakerType: speaker
        }
      });
      
      if (response.error) {
        console.error(`‚ùå Erro na fun√ß√£o save-live-transcript:`, response.error);
        return;
      }
      
      // Atualizar estado local apenas ap√≥s confirma√ß√£o do save
      if (speaker === 'user') {
        setUserTranscript(prev => prev + (prev ? '\n' : '') + text);
      } else {
        setAiTranscript(prev => prev + (prev ? '\n' : '') + text);
      }
      
      console.log(`‚úÖ Transcript ${speaker} salvo:`, text.substring(0, 50), response.data);
    } catch (error) {
      console.error(`‚ùå Erro ao salvar transcript ${speaker}:`, error);
    }
  };

  // Adicionamos uma refer√™ncia ao tempo de in√≠cio
  const startTimeRef = useRef<number | null>(null);

  async function startCall({ track, scenario, systemPrompt, voiceId }: StartOpts) {
    try {
      setStatus("connecting");
      console.log("=== INICIANDO CHAMADA ===");
      console.log("Par√¢metros:", { track, scenario, systemPrompt, voiceId });
      
      // Marcar tempo de in√≠cio real
      startTimeRef.current = Date.now();
      
      // Criar nova sess√£o no banco
      const sessionId = crypto.randomUUID();
      setCurrentSessionId(sessionId);
      
      const { data: { user } } = await supabase.auth.getUser();
      
      const { error: sessionError } = await supabase
        .from('sessions_live')
        .insert({
          id: sessionId,
          track: track,
          user_id: user?.id,
          duration_ms: 0,
          metadata: {
            scenario_title: scenario || 'Simula√ß√£o Live',
            system_prompt: systemPrompt,
            voice_id: voiceId,
            started_at: new Date().toISOString(),
            start_timestamp: startTimeRef.current
          }
        });
        
      if (sessionError) {
        console.error('‚ùå Erro ao criar sess√£o:', sessionError);
        throw sessionError;
      }
      
      console.log('‚úÖ Sess√£o criada:', sessionId);
      
      // 1) token ef√™mero - usando nova edge function
      console.log("Fazendo requisi√ß√£o para token...");
      const tokenRes = await fetch("https://roboonbdessuipvcpgve.supabase.co/functions/v1/realtime-token", {
        method: "POST",
        headers: { 
          "Content-Type": "application/json",
          "apikey": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJvYm9vbmJkZXNzdWlwdmNwZ3ZlIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTYyMTA0NDAsImV4cCI6MjA3MTc4NjQ0MH0.lbkilZMpgno7M3NVBhE3P0MoKJEXZIIuLLPCwkN120k",
          "Authorization": "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InJvYm9vbmJkZXNzdWlwdmNwZ3ZlIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTYyMTA0NDAsImV4cCI6MjA3MTc4NjQ0MH0.lbkilZMpgno7M3NVBhE3P0MoKJEXZIIuLLPCwkN120k"
        },
        body: JSON.stringify({
          track,
          scenario,
          system_prompt: systemPrompt,
          voice_id: voiceId
        })
      });
      
      console.log("Status da resposta:", tokenRes.status);
      
      if (!tokenRes.ok) {
        const errorText = await tokenRes.text();
        console.error("Erro na requisi√ß√£o do token:", errorText);
        throw new Error(`Falha ao obter token: ${tokenRes.status} - ${errorText}`);
      }
      
      const tokenJson = await tokenRes.json();
      console.log("Conte√∫do da resposta:", tokenJson);
      
      const EPHEMERAL = tokenJson?.client_secret?.value || tokenJson?.token;
      if (!EPHEMERAL) {
        console.error("Token ef√™mero n√£o encontrado na resposta:", tokenJson);
        throw new Error("Token ef√™mero ausente");
      }
      
      console.log("‚úì Token obtido com sucesso, iniciando WebRTC...");

      // 2) captura microfone
      console.log("Solicitando acesso ao microfone...");
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      localStreamRef.current = stream;
      console.log("Microfone capturado");

      // 3) RTCPeerConnection
      const pc = new RTCPeerConnection();
      pcRef.current = pc;
      console.log("RTCPeerConnection criado");

      // remote track -> audio element
      pc.ontrack = (event) => {
        console.log("Track remoto recebido");
        const [remoteStream] = event.streams;
        if (remoteAudioRef.current) {
          remoteAudioRef.current.srcObject = remoteStream;
        }
      };

      // add local audio track
      stream.getAudioTracks().forEach(track => pc.addTrack(track, stream));
      console.log("Track local adicionado");

      // 4) datachannel para capturar eventos e transcri√ß√µes
      const dataChannel = pc.createDataChannel("oai-events");
      dataChannelRef.current = dataChannel;
      
      dataChannel.addEventListener("open", () => {
        console.log("‚úÖ DataChannel aberto - pronto para receber transcri√ß√µes");
      });
      
      let currentAiTranscript = '';
      
      dataChannel.addEventListener("message", (event) => {
        try {
          const data = JSON.parse(event.data);
          console.log("üì° Evento recebido:", data.type, data);
          
          // Capturar transcri√ß√µes em tempo real
          if (data.type === 'input_audio_buffer.speech_started') {
            console.log("üé§ Usu√°rio come√ßou a falar");
          }
          
          if (data.type === 'input_audio_buffer.speech_stopped') {
            console.log("üé§ Usu√°rio parou de falar");
          }
          
          // Eventos de transcri√ß√£o do usu√°rio - M√öLTIPLOS TIPOS POSS√çVEIS
          if (data.type === 'conversation.item.input_audio_transcription.completed') {
            console.log("üìù Transcri√ß√£o do usu√°rio completa (completed):", data.transcript);
            if (data.transcript?.trim()) {
              saveTranscript(data.transcript, 'user');
            }
          }
          
          if (data.type === 'input_audio_transcription.completed') {
            console.log("üìù Transcri√ß√£o do usu√°rio completa (input):", data.transcript);
            if (data.transcript?.trim()) {
              saveTranscript(data.transcript, 'user');
            }
          }
          
          if (data.type === 'conversation.item.input_audio_transcription.failed') {
            console.error("‚ùå Falha na transcri√ß√£o do usu√°rio:", data.error);
          }
          
          // Eventos de transcri√ß√£o da IA
          if (data.type === 'response.audio_transcript.delta') {
            console.log("üìù Delta da IA:", data.delta);
            currentAiTranscript += data.delta;
            setAiTranscript(prev => prev + data.delta);
          }
          
          if (data.type === 'response.audio_transcript.done') {
            console.log("üìù Transcri√ß√£o da IA completa:", currentAiTranscript);
            if (currentAiTranscript.trim()) {
              saveTranscript(currentAiTranscript, 'ai');
              currentAiTranscript = ''; // Reset para pr√≥xima resposta
            }
          }
          
          // Log para debug de timeouts e limites
          if (data.type === 'session.updated') {
            console.log("‚öôÔ∏è Sess√£o atualizada:", data.session);
          }
          
          if (data.type === 'error') {
            console.error("‚ùå Erro na sess√£o WebRTC:", data.error);
          }
          
          if (data.type === 'rate_limits.updated') {
            console.log("‚è±Ô∏è Rate limits atualizados:", data.rate_limits);
          }
          
          // Logs expandidos para debug de eventos de transcri√ß√£o
          if (data.type.includes('transcript') || 
              data.type.includes('audio') || 
              data.type.includes('input_audio') ||
              data.type.includes('conversation.item')) {
            console.log("üîç Evento detalhado:", {
              type: data.type,
              transcript: data.transcript,
              delta: data.delta,
              content: data.content,
              fullEvent: data
            });
          }
          
        } catch (error) {
          console.error("‚ùå Erro ao processar evento:", error);
        }
      });

      // 5) negocia SDP via fetch (WebRTC com OpenAI)
      console.log("Criando oferta SDP...");
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      console.log("Enviando SDP para OpenAI...");
      const sdpResponse = await fetch(
        "https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-12-17",
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${EPHEMERAL}`,
            "Content-Type": "application/sdp"
          },
          body: offer.sdp
        }
      );
      
      console.log("Resposta SDP:", sdpResponse.status);
      if (!sdpResponse.ok) {
        const errorText = await sdpResponse.text();
        console.error("Erro SDP:", errorText);
        throw new Error(`Falha na negocia√ß√£o SDP: ${sdpResponse.status} - ${errorText}`);
      }

      const answer = { type: "answer", sdp: await sdpResponse.text() } as RTCSessionDescriptionInit;
      await pc.setRemoteDescription(answer);
      console.log("Conex√£o WebRTC estabelecida");

      setStatus("connected");
      
      // Aguardar conex√£o estabelecer e configurar eventos
      setTimeout(() => {
        console.log("‚úÖ Conex√£o WebRTC totalmente estabelecida");
        console.log("üé§ Captura de transcri√ß√µes ativada para sess√£o:", sessionId);
        // A IA deveria iniciar automaticamente com as instru√ß√µes do realtime-token
      }, 1000);
    } catch (error) {
      console.error("=== ERRO DETALHADO ===");
      console.error("Tipo do erro:", error);
      console.error("Mensagem:", error instanceof Error ? error.message : String(error));
      console.error("Stack:", error instanceof Error ? error.stack : "N/A");
      
      // Limpar recursos se houver erro
      const pc = pcRef.current;
      if (pc) {
        pc.close();
        pcRef.current = null;
      }
      
      const ls = localStreamRef.current;
      if (ls) {
        ls.getTracks().forEach(t => t.stop());
        localStreamRef.current = null;
      }
      
      setStatus("ended");
      
      // Re-throw para que possa ser tratado pela UI se necess√°rio
      throw error;
    }
  }

  function toggleMute() {
    setMuted((m) => {
      const next = !m;
      const tracks = localStreamRef.current?.getAudioTracks?.() || [];
      for (const t of tracks) t.enabled = !next;
      return next;
    });
  }

  async function endCall() {
    console.log("üèÅ Finalizando chamada e salvando dados finais...");
    
    // Finalizar a sess√£o no banco antes de fechar conex√µes
    if (currentSessionId && startTimeRef.current) {
      try {
        const actualDuration = Date.now() - startTimeRef.current;
        console.log('üïê Dura√ß√£o real calculada:', actualDuration, 'ms');
        
        const { error: finalizeError } = await supabase
          .from('sessions_live')
          .update({
            completed_at: new Date().toISOString(),
            duration_ms: actualDuration,
            transcript_user: userTranscript || null,
            transcript_ai: aiTranscript || null,
            metadata: {
              completed: true,
              final_user_transcript: userTranscript,
              final_ai_transcript: aiTranscript,
              total_interactions: (userTranscript.split('\n').length + aiTranscript.split('\n').length),
              actual_duration: actualDuration
            }
          })
          .eq('id', currentSessionId);
          
        if (finalizeError) {
          console.error('‚ùå Erro ao finalizar sess√£o:', finalizeError);
        } else {
          console.log('‚úÖ Sess√£o finalizada com sucesso:', currentSessionId, 'dura√ß√£o:', actualDuration);
        }
      } catch (error) {
        console.error('‚ùå Erro ao salvar dados finais:', error);
      }
    }
    
    setStatus("ended");
    
    // Fechar conex√µes
    const pc = pcRef.current;
    pc?.getSenders().forEach(s => s.track && s.track.stop());
    pc?.close();
    pcRef.current = null;

    const ls = localStreamRef.current;
    ls?.getTracks().forEach(t => t.stop());
    localStreamRef.current = null;

    if (remoteAudioRef.current) {
      (remoteAudioRef.current as any).srcObject = null;
    }
    
    // Limpar estado
    dataChannelRef.current = null;
    setCurrentSessionId(null);
    setUserTranscript("");
    setAiTranscript("");
    startTimeRef.current = null;
  }

  return { 
    startCall, 
    endCall, 
    toggleMute, 
    muted, 
    status,
    remoteAudioElement: remoteAudioRef.current,
    localStream: localStreamRef.current,
    currentSessionId,
    userTranscript,
    aiTranscript
  };
}